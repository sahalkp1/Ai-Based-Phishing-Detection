{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"},
  "colab": {"provenance": []}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "md-title",
   "source": [
    "# ðŸ”— Phishing URL Detection â€” Feature Analysis\n",
    "### Phase 4 | Logistic Regression + Random Forest\n",
    "---\n",
    "ðŸ“‚ **Dataset:** `phishing_url_ml_ready.csv`\n",
    "- 48,812 URLs â€” perfectly balanced (50% phishing / 50% legitimate)\n",
    "- 27 pre-extracted features (URL length, HTTPS, @ symbol, domain entropy, etc.)\n",
    "- Label: `1` = Phishing, `0` = Legitimate\n",
    "\n",
    "> âœ… No GPU needed for this phase â€” runs on CPU just fine!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "md-s1",
   "source": ["## âœ… Step 1 â€” Install & Import Libraries"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "install",
   "outputs": [],
   "source": [
    "!pip install scikit-learn seaborn -q\n",
    "print('âœ… Libraries ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "imports",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report,\n",
    "    confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "print('âœ… All imports done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "md-s2",
   "source": ["## âœ… Step 2 â€” Upload Dataset"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "upload",
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "print('ðŸ“‚ Upload phishing_url_ml_ready.csv')\n",
    "uploaded = files.upload()\n",
    "print('âœ… Uploaded:', list(uploaded.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "md-s3",
   "source": ["## âœ… Step 3 â€” Load & Explore Data"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "load",
   "outputs": [],
   "source": [
    "df = pd.read_csv('phishing_url_ml_ready.csv')\n",
    "\n",
    "print('Shape:', df.shape)\n",
    "print('\\nLabel distribution:')\n",
    "print(df['label'].value_counts())\n",
    "print('\\nFeature overview:')\n",
    "display(df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "explore",
   "outputs": [],
   "source": [
    "# Compare key features between phishing and legitimate URLs\n",
    "key_features = ['url_length', 'is_https', 'has_ip_address',\n",
    "                'num_at_signs', 'num_hyphens', 'domain_entropy',\n",
    "                'has_suspicious_tld', 'num_suspicious_words']\n",
    "\n",
    "comparison = df.groupby('label')[key_features].mean().round(3)\n",
    "comparison.index = ['Legitimate (0)', 'Phishing (1)']\n",
    "\n",
    "print('ðŸ“Š Average feature values by class:')\n",
    "display(comparison.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "visualise",
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    for label, color, name in [(0, 'steelblue', 'Legitimate'), (1, 'tomato', 'Phishing')]:\n",
    "        axes[i].hist(\n",
    "            df[df['label'] == label][feature],\n",
    "            bins=30, alpha=0.6, color=color, label=name\n",
    "        )\n",
    "    axes[i].set_title(feature, fontsize=11)\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Feature Distributions: Phishing vs Legitimate URLs', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "md-s4",
   "source": ["## âœ… Step 4 â€” Prepare Data for ML"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "prepare",
   "outputs": [],
   "source": [
    "# Separate features and label\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "print(f'Features: {len(feature_names)}')\n",
    "print(f'Samples:  {len(X)}')\n",
    "\n",
    "# Train / Test split â€” 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f'\\nTrain: {len(X_train)} | Test: {len(X_test)}')\n",
    "\n",
    "# Scale features â€” required for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "print('âœ… Data ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "md-s5",
   "source": ["## âœ… Step 5 â€” Model 1: Logistic Regression"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "logreg",
   "outputs": [],
   "source": [
    "print('Training Logistic Regression...')\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "lr_preds = lr_model.predict(X_test_scaled)\n",
    "lr_probs = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "lr_acc = accuracy_score(y_test, lr_preds)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "\n",
    "print(f'\\nâœ… Logistic Regression Results:')\n",
    "print(f'   Accuracy:  {lr_acc:.4f} ({lr_acc*100:.2f}%)')\n",
    "print(f'   ROC-AUC:   {lr_auc:.4f}')\n",
    "print(f'\\n{classification_report(y_test, lr_preds, target_names=[\"Legitimate\", \"Phishing\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "md-s6",
   "source": ["## âœ… Step 6 â€” Model 2: Random Forest"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "rf",
   "outputs": [],
   "source": [
    "print('Training Random Forest... (takes ~1 min)')\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # use all CPU cores\n",
    ")\n",
    "rf_model.fit(X_train, y_train)  # Random Forest doesn't need scaling\n",
    "\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "rf_probs = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "rf_acc = accuracy_score(y_test, rf_preds)\n",
    "rf_auc = roc_auc_score(y_test, rf_probs)\n",
    "\n",
    "print(f'\\nâœ… Random Forest Results:')\n",
    "print(f'   Accuracy:  {rf_acc:.4f} ({rf_acc*100:.2f}%)')\n",
    "print(f'   ROC-AUC:   {rf_auc:.4f}')\n",
    "print(f'\\n{classification_report(y_test, rf_preds, target_names=[\"Legitimate\", \"Phishing\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "md-s7",
   "source": ["## âœ… Step 7 â€” Compare Both Models"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "compare",
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# â”€â”€ Confusion Matrix: Logistic Regression â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cm_lr = confusion_matrix(y_test, lr_preds)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Legitimate', 'Phishing'],\n",
    "            yticklabels=['Legitimate', 'Phishing'],\n",
    "            annot_kws={'size': 13})\n",
    "axes[0].set_title(f'Logistic Regression\\nAccuracy: {lr_acc*100:.2f}%', fontsize=12)\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "# â”€â”€ Confusion Matrix: Random Forest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cm_rf = confusion_matrix(y_test, rf_preds)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Legitimate', 'Phishing'],\n",
    "            yticklabels=['Legitimate', 'Phishing'],\n",
    "            annot_kws={'size': 13})\n",
    "axes[1].set_title(f'Random Forest\\nAccuracy: {rf_acc*100:.2f}%', fontsize=12)\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "# â”€â”€ ROC Curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probs)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)\n",
    "\n",
    "axes[2].plot(fpr_lr, tpr_lr, color='steelblue', lw=2,\n",
    "             label=f'Logistic Regression (AUC = {lr_auc:.3f})')\n",
    "axes[2].plot(fpr_rf, tpr_rf, color='tomato', lw=2,\n",
    "             label=f'Random Forest (AUC = {rf_auc:.3f})')\n",
    "axes[2].plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "axes[2].set_title('ROC Curve Comparison', fontsize=12)\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# â”€â”€ Summary table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('\\n' + '='*45)\n",
    "print('         MODEL COMPARISON SUMMARY')\n",
    "print('='*45)\n",
    "print(f'{\"Model\":<25} {\"Accuracy\":>10} {\"AUC\":>10}')\n",
    "print('-'*45)\n",
    "print(f'{\"Logistic Regression\":<25} {lr_acc*100:>9.2f}% {lr_auc:>10.4f}')\n",
    "print(f'{\"Random Forest\":<25} {rf_acc*100:>9.2f}% {rf_auc:>10.4f}')\n",
    "print('='*45)\n",
    "winner = 'Random Forest' if rf_acc > lr_acc else 'Logistic Regression'\n",
    "print(f'ðŸ† Best model: {winner}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "md-s8",
   "source": ["## âœ… Step 8 â€” Feature Importance (Random Forest)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "feature-importance",
   "outputs": [],
   "source": [
    "importances = pd.Series(rf_model.feature_importances_, index=feature_names)\n",
    "importances = importances.sort_values(ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['tomato' if v > importances.median() else 'steelblue'\n",
    "          for v in importances.values]\n",
    "importances.plot(kind='barh', color=colors, edgecolor='black', linewidth=0.5)\n",
    "plt.title('Feature Importance â€” Random Forest', fontsize=14)\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nTop 5 most important features:')\n",
    "for i, (feat, score) in enumerate(importances.sort_values(ascending=False).head(5).items(), 1):\n",
    "    print(f'  {i}. {feat}: {score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "md-s9",
   "source": ["## âœ… Step 9 â€” Live URL Prediction"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "predict-fn",
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def extract_features(url):\n",
    "    \"\"\"Extract the same 27 features used during training.\"\"\"\n",
    "    f = {}\n",
    "    f['url_length']           = len(url)\n",
    "    f['num_dots']             = url.count('.')\n",
    "    f['num_hyphens']          = url.count('-')\n",
    "    f['num_underscores']      = url.count('_')\n",
    "    f['num_slashes']          = url.count('/')\n",
    "    f['num_question_marks']   = url.count('?')\n",
    "    f['num_equal_signs']      = url.count('=')\n",
    "    f['num_at_signs']         = url.count('@')\n",
    "    f['num_ampersands']       = url.count('&')\n",
    "    f['num_percent']          = url.count('%')\n",
    "    f['num_digits']           = sum(c.isdigit() for c in url)\n",
    "    f['digit_ratio']          = f['num_digits'] / len(url) if len(url) > 0 else 0\n",
    "    suspicious_words = ['login','signin','verify','secure','account',\n",
    "                        'update','banking','confirm','password','pay',\n",
    "                        'free','lucky','win','bonus','click']\n",
    "    f['num_suspicious_words'] = sum(w in url.lower() for w in suspicious_words)\n",
    "    f['has_ip_address']       = int(bool(re.search(r'(https?://)?(\\d{1,3}\\.){3}\\d{1,3}', url)))\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(url if url.startswith('http') else 'http://' + url)\n",
    "    except:\n",
    "        parsed = urlparse('')\n",
    "\n",
    "    f['is_https']             = int(parsed.scheme == 'https')\n",
    "    netloc                    = parsed.netloc or ''\n",
    "    domain                    = netloc.split(':')[0]\n",
    "    f['domain_length']        = len(domain)\n",
    "    f['num_subdomains']       = max(0, len(domain.split('.')) - 2)\n",
    "    parts                     = domain.split('.')\n",
    "    tld                       = parts[-1].lower() if len(parts) >= 2 else ''\n",
    "    suspicious_tlds           = ['ru','xyz','tk','ml','ga','cf','gq','top',\n",
    "                                  'work','click','link','info','biz','online']\n",
    "    f['has_suspicious_tld']   = int(tld in suspicious_tlds)\n",
    "    path                      = parsed.path or ''\n",
    "    f['path_length']          = len(path)\n",
    "    f['path_depth']           = path.count('/')\n",
    "    f['has_exe_extension']    = int(bool(re.search(r'\\.(exe|sh|bat|cmd|msi|ps1|vbs|js)$', path.lower())))\n",
    "    f['has_php']              = int('.php' in path.lower())\n",
    "    query                     = parsed.query or ''\n",
    "    f['query_length']         = len(query)\n",
    "    f['num_query_params']     = query.count('&') + 1 if query else 0\n",
    "    port                      = parsed.port\n",
    "    f['has_non_standard_port']= int(port is not None and port not in [80, 443])\n",
    "    if domain:\n",
    "        probs = [domain.count(c) / len(domain) for c in set(domain)]\n",
    "        f['domain_entropy']   = -sum(p * math.log2(p) for p in probs if p > 0)\n",
    "    else:\n",
    "        f['domain_entropy']   = 0\n",
    "    f['tld_encoded']          = hash(tld) % 100  # simple encoding\n",
    "    return f\n",
    "\n",
    "\n",
    "def predict_url(url):\n",
    "    \"\"\"Predict whether a URL is phishing or legitimate.\"\"\"\n",
    "    features = extract_features(url)\n",
    "    X_new = pd.DataFrame([features])[feature_names]\n",
    "    X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "    # Random Forest prediction (best model)\n",
    "    pred  = rf_model.predict(X_new)[0]\n",
    "    proba = rf_model.predict_proba(X_new)[0]\n",
    "\n",
    "    label = 'ðŸš¨ PHISHING' if pred == 1 else 'âœ… LEGITIMATE'\n",
    "    print(f'URL:        {url}')\n",
    "    print(f'Prediction: {label}')\n",
    "    print(f'Confidence: {proba.max()*100:.2f}%')\n",
    "    print(f'Legitimate: {proba[0]*100:.2f}%  |  Phishing: {proba[1]*100:.2f}%')\n",
    "    print('-' * 65)\n",
    "\n",
    "\n",
    "print('='*65)\n",
    "print('          LIVE URL PHISHING DETECTION DEMO')\n",
    "print('='*65)\n",
    "\n",
    "predict_url('https://www.google.com')\n",
    "predict_url('http://paypal-secure-login.verify-account.tk/update')\n",
    "predict_url('https://github.com/features')\n",
    "predict_url('http://192.168.1.1:8080/admin/login.php')\n",
    "predict_url('http://free-iphone-winner.click/claim?user=you&prize=yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "custom-url",
   "outputs": [],
   "source": [
    "# â”€â”€ Try YOUR OWN URL here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "my_url = \"https://paste-any-url-here.com\"\n",
    "\n",
    "predict_url(my_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "md-s10",
   "source": ["## âœ… Step 10 â€” Save Both Models"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "save",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save Random Forest\n",
    "with open('url_rf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "# Save Logistic Regression\n",
    "with open('url_lr_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "\n",
    "# Save scaler (needed for Logistic Regression predictions)\n",
    "with open('url_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print('âœ… Models saved!')\n",
    "\n",
    "# Zip and download all\n",
    "!zip url_models.zip url_rf_model.pkl url_lr_model.pkl url_scaler.pkl\n",
    "\n",
    "from google.colab import files\n",
    "files.download('url_models.zip')\n",
    "print('âœ… Download started!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "md-done",
   "source": [
    "---\n",
    "## ðŸŽ‰ Phase 4 Complete!\n",
    "\n",
    "| Step | Task | Status |\n",
    "|------|------|--------|\n",
    "| 1 | Install libraries | âœ… |\n",
    "| 2 | Upload dataset | âœ… |\n",
    "| 3 | Load & explore (48,812 URLs) | âœ… |\n",
    "| 4 | Prepare features + split 80/20 | âœ… |\n",
    "| 5 | Train Logistic Regression | âœ… |\n",
    "| 6 | Train Random Forest | âœ… |\n",
    "| 7 | Compare models + ROC curve | âœ… |\n",
    "| 8 | Feature importance chart | âœ… |\n",
    "| 9 | Live URL prediction function | âœ… |\n",
    "| 10 | Save & download models | âœ… |\n",
    "\n",
    "**Expected results:**\n",
    "- Logistic Regression: ~88â€“92% accuracy\n",
    "- Random Forest: ~96â€“98% accuracy\n",
    "\n",
    "**Files saved:**\n",
    "- `url_rf_model.pkl` â€” Random Forest model\n",
    "- `url_lr_model.pkl` â€” Logistic Regression model  \n",
    "- `url_scaler.pkl` â€” Feature scaler\n",
    "---"
   ]
  }
 ]
}
